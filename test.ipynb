{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniconda/base/envs/xlstm/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniconda/base/envs/xlstm/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/xlstm/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniconda/base/envs/xlstm/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/xlstm/lib/python3.12/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/homebrew/Caskroom/miniconda/base/envs/xlstm/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/Caskroom/miniconda/base/envs/xlstm/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/envs/xlstm/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/xlstm/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniconda/base/envs/xlstm/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/xlstm/lib/python3.12/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/xlstm/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/xlstm/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/xlstm/lib/python3.12/site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/xlstm/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/xlstm/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/xlstm/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/xlstm/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/Caskroom/miniconda/base/envs/xlstm/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/xlstm/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([12, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/xlstm/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([15])) that is different to the input size (torch.Size([12, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/xlstm/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([12, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 20144.5732421875, Val Loss: 15501.62158203125\n",
      "Epoch 2/20, Train Loss: 30347.456640625, Val Loss: 15554.00439453125\n",
      "Epoch 3/20, Train Loss: 37895.578515625, Val Loss: 15588.64306640625\n",
      "Epoch 4/20, Train Loss: 29833.8888671875, Val Loss: 15583.36181640625\n",
      "Epoch 5/20, Train Loss: 28615.59765625, Val Loss: 15562.40673828125\n",
      "Epoch 6/20, Train Loss: 20060.254296875, Val Loss: 15542.798828125\n",
      "Epoch 7/20, Train Loss: 18001.414453125, Val Loss: 15537.94140625\n",
      "Epoch 8/20, Train Loss: 25280.7890625, Val Loss: 15529.34619140625\n",
      "Epoch 9/20, Train Loss: 38466.718359375, Val Loss: 15514.3740234375\n",
      "Epoch 10/20, Train Loss: 29036.2955078125, Val Loss: 15484.93603515625\n",
      "Epoch 11/20, Train Loss: 17545.4921875, Val Loss: 15473.69921875\n",
      "Epoch 12/20, Train Loss: 24718.3123046875, Val Loss: 15485.47998046875\n",
      "Epoch 13/20, Train Loss: 27310.60615234375, Val Loss: 15498.85986328125\n",
      "Epoch 14/20, Train Loss: 29764.6052734375, Val Loss: 15491.20556640625\n",
      "Epoch 15/20, Train Loss: 24537.98828125, Val Loss: 15481.57763671875\n",
      "Epoch 16/20, Train Loss: 24187.7205078125, Val Loss: 15491.25341796875\n",
      "Epoch 17/20, Train Loss: 27418.6634765625, Val Loss: 15532.10302734375\n",
      "Epoch 18/20, Train Loss: 24532.85703125, Val Loss: 15542.16748046875\n",
      "Epoch 19/20, Train Loss: 27146.069921875, Val Loss: 15557.830078125\n",
      "Epoch 20/20, Train Loss: 19830.66171875, Val Loss: 15645.31201171875\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 114\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Evaluate model on the test set\u001b[39;00m\n\u001b[1;32m    113\u001b[0m test_data \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m-\u001b[39m(test_size \u001b[38;5;241m+\u001b[39m seq_len):]\n\u001b[0;32m--> 114\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model_full_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Plot the results\u001b[39;00m\n\u001b[1;32m    117\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "Cell \u001b[0;32mIn[3], line 50\u001b[0m, in \u001b[0;36mevaluate_model_full_sequence\u001b[0;34m(model, data, device, seq_len)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seq_len, \u001b[38;5;28mlen\u001b[39m(data)):\n\u001b[1;32m     49\u001b[0m     input_seq \u001b[38;5;241m=\u001b[39m data[i\u001b[38;5;241m-\u001b[39mseq_len:i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape: (seq_len, 1, 1)\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     output, _, _, _, _ \u001b[38;5;241m=\u001b[39m model(input_seq)\n\u001b[1;32m     51\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     52\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(prediction)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 2)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from xlstm import xLSTM\n",
    "\n",
    "# Define the dataset class\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index:index + self.seq_len]\n",
    "        y = self.data[index + self.seq_len]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Load the data\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path, usecols=[1])\n",
    "    data = df.values.astype(float).reshape(-1)\n",
    "    return data\n",
    "\n",
    "# Define the evaluation function for validation\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            output, _ = model(x_batch.unsqueeze(-1))\n",
    "            loss = criterion(output[-1], y_batch)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Define the evaluation function for full sequence\n",
    "def evaluate_model_full_sequence(model, data, device, seq_len):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    data = torch.tensor(data, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(seq_len, len(data)):\n",
    "            input_seq = data[i-seq_len:i].unsqueeze(1)  # Shape: (seq_len, 1, 1)\n",
    "            output, _ = model(input_seq)\n",
    "            prediction = output[-1].item()\n",
    "            predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "# Define the training function\n",
    "def train_model(model, train_dataloader, val_dataloader, criterion, optimizer, device, num_epochs=20):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for x_batch, y_batch in train_dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, _ = model(x_batch.unsqueeze(-1))\n",
    "            loss = criterion(output[-1], y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        train_losses.append(total_train_loss / len(train_dataloader))\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss = evaluate_model(model, val_dataloader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_losses[-1]}, Val Loss: {val_losses[-1]}\")\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Parameters\n",
    "file_path = 'AirPassengers.csv'  # Change this to your dataset path\n",
    "seq_len = 12\n",
    "batch_size = 16\n",
    "hidden_size = 128\n",
    "num_heads = 1\n",
    "layers = ['m', 's', 'm']\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load data\n",
    "data = load_data(file_path)\n",
    "dataset = TimeSeriesDataset(data, seq_len)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "train_size = int(len(dataset) * 0.6)\n",
    "val_size = int(len(dataset) * 0.2)\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define model\n",
    "model = xLSTM(input_size=1, hidden_size=hidden_size, num_heads=num_heads, layers=layers).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train model\n",
    "train_losses, val_losses = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, device, num_epochs)\n",
    "\n",
    "# Evaluate model on the test set\n",
    "test_data = data[-(test_size + seq_len):]\n",
    "predictions = evaluate_model_full_sequence(model, test_data, device, seq_len)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(data, label='Actual')\n",
    "plt.plot(range(len(data) - test_size, len(data)), predictions, label='Predicted')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Number of Passengers')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
